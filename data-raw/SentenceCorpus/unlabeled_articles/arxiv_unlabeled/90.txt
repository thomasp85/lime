### abstract ###
Regularization by the sum of singular values, also referred to as the   trace norm , is a popular technique for estimating  low rank rectangular matrices
In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss
We also  provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled
### introduction ###
In recent years, regularization by various non Euclidean  norms has seen considerable interest
In particular, in the context of linear supervised learning, norms such as the  SYMBOL -norm may induce sparse loading vectors, i e , loading vectors with low cardinality or  SYMBOL -norm
Such regularization schemes, also known as the Lasso~ CITATION  for least-square regression, come with efficient path following algorithms~ CITATION
Moreover, recent work has studied conditions under which such procedures consistently estimate the sparsity pattern of the loading vector~ CITATION
When learning on rectangular matrices, the rank is a natural extension of the cardinality, and the sum of singular values, also known as the trace norm or the nuclear norm, is the natural extension of the  SYMBOL -norm; indeed, as the  SYMBOL -norm is the convex envelope of the  SYMBOL -norm on the unit ball (i e , the largest lower bounding convex function)~ CITATION , the trace norm is the convex envelope of the rank over the unit ball of the spectral norm~ CITATION
In practice, it leads to low rank solutions~ CITATION  and has seen recent increased interest in the context of collaborative filtering~ CITATION , multi-task learning~ CITATION  or classification with multiple classes~ CITATION
In this paper, we consider the rank consistency of trace norm regularization with the square loss, i e , if the data were actually generated by a low-rank matrix, will the matrix and its rank be consistently estimated
In \mysec{consistency}, we provide necessary and sufficient conditions for the rank consistency that are extensions of corresponding results for the Lasso~ CITATION   and the group Lasso~ CITATION
We do so under two sets of sampling assumptions detailed in \mysec{assumptions}: a full  iid  assumption and a non  iid  assumption which is natural in the context of collaborative filtering
As for the Lasso and the group Lasso, the necessary condition implies that such procedures do not always estimate the rank correctly; following the adaptive version of the Lasso and group Lasso~ CITATION , we design an adaptive version to achieve  SYMBOL -consistency and rank consistency, with no consistency conditions
Finally,  in \mysec{algorithms}, we present a smoothing approach to convex optimization with the trace norm, while  in \mysec{simulations}, we show simulations on toy examples to illustrate the consistency results
