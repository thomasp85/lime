% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/plot_text.R
\name{plot_text_explanations}
\alias{plot_text_explanations}
\title{Plot text explanations}
\usage{
plot_text_explanations(explanations, ...)
}
\arguments{
\item{explanations}{object returned by the \link{lime.character} function.}

\item{...}{parameters passed to \link{sizingPolicy}}
}
\description{
Highlight words which explains a prediction.
}
\examples{
\dontrun{
# Explaining a model based on text data

# Purpose is to classify sentences from scientific publications
# and find those where the team writes about their own work
# (category OWNX in the provided dataset).

library(lime)
library(text2vec)
library(xgboost)

data(train_sentences)
data(test_sentences)

get_matrix <- function(text) {
  it <- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}

dtm_train = get_matrix(train_sentences$text)

xgb_model <- xgb.train(list(max_depth = 7, eta = 0.1, objective = "binary:logistic",
                 eval_metric = "error", nthread = 1),
                 xgb.DMatrix(dtm_train, label = train_sentences$class.text == "OWNX"),
                 nrounds = 50)

sentences <- head(test_sentences[test_sentences$class.text == "OWNX", "text"], 5)
explainer <- lime(sentences, xgb_model, get_matrix)
explanations <- explain(sentences, explainer, n_labels = 1, n_features = 2)

# We can see that many explanations are based
# on the presence of the word `we` in the sentences
# which makes sense regarding the task.
print(explanations)

# We can also print the explanations to see the selected words in their context.
plot_text_explanations(explanations)
}

}
