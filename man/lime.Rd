% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/character.R, R/dataframe.R, R/lime.R
\name{lime.character}
\alias{lime.character}
\alias{lime.data.frame}
\alias{lime}
\title{Create a model explanation function based on training data}
\usage{
\method{lime}{character}(x, model, preprocess,
  tokenization = default_tokenize, keep_word_position = FALSE,
  n_permutations = 5000, number_features_explain = 5,
  feature_selection_method = "auto", labels = NULL, n_labels = NULL,
  prediction = default_predict, ...)

\method{lime}{data.frame}(x, model, bin_continuous = TRUE, n_bins = 4,
  quantile_bins = TRUE, kernel_width = NULL, ...)

lime(x, model, ...)
}
\arguments{
\item{x}{The training data used for training the model that should be
explained.}

\item{model}{The model whose output should be explained}

\item{preprocess}{Function to transform \code{\link{character}} vector to feature provided to the model to explain}

\item{tokenization}{function used to tokenize text}

\item{keep_word_position}{set to \code{\link{TRUE}} if to keep order of words.}

\item{n_permutations}{number of permutations to perform. More gives better explanation up to a point where it is not usefull and takes too much time. (5000)}

\item{number_features_explain}{number of features used in the explanation. (\code{5})}

\item{feature_selection_method}{method to select the best features. (\code{"auto"})

One of:
\itemize{
\item{"auto"}{ : If n_features <= 6 use \code{"forward_selection"} else use \code{"highest_weights"}.}
\item{"none"} {Ignore \code{n_features} and use all features.}
\item{"forward_selection"} {: Add one feature at a time until \code{n_features} is
reached, based on quality of a ridge regression model.}
\item{"highest_weights"} {: Fit a ridge regression and select the \code{n_features} with
the highest absolute weight.}
\item{"lasso_path"} {: Fit a lasso model and choose the \code{n_features} whose lars
path converge to zero the latest.}
\item{"tree"} {: Fit a tree to select \code{n_features}. It requires XGBoost.}
}}

\item{labels}{name of the label to explain (use only when model to explain predictions includes names as \code{\link{data.frame}} column names, like with \code{link{caret}}). (\code{\link{NULL}}).}

\item{n_labels}{instead of labels, number of labels to explain. (\code{\link{NULL}})}

\item{prediction}{function used to perform the prediction. Should have 2 variables, first for the \code{\link{character}} vector, second for the \code{model}. Should return a \code{\link{data.frame}} with the predictions.}

\item{...}{Arguments passed on to methods}

\item{bin_continuous}{Should continuous variables be binned when making the explanation}

\item{n_bins}{The number of bins for continuous variables if \code{bin_continuous = TRUE}}

\item{quantile_bins}{Should the bins be based on \code{n_bins} quantiles or spread evenly over the range of the training data}

\item{kernel_width}{The width of the kernel used for converting the distances to permutations into weights}
}
\value{
Return a function. To make only one call you can perform a currying like in \code{lime(...)(...)}.

A function taking the following arguments:
\itemize{
\item \code{cases}: Data of the same format as \code{x} that needs to be explained
\item \code{labels}: The prediction(s) that needs to be explained
\item \code{n_labels}: Alternative to \code{labels}, the number of predictions to explain,
selected by their probability.
\item \code{n_features}: The number of features to use in the explanaition.
\item \code{n_permutations}: The number of permutations to make on each row in \code{cases}
\item \code{dist_fun}: The distance measure to use for weighting the permutations
\item \code{feature_select}: The method to use for feature selection. One of:
\itemize{
\item \code{"auto"}: If \code{n_features <= 6} use \code{"forward_selection"} else use \code{"highest_weights"}.
\item \code{"none"}: Ignore \code{n_features} and use all features.
\item \code{"forward_selection"}: Add one feature at a time until \code{n_features} is
reached, based on quality of a ridge regression model.
\item \code{"highest_weights"}: Fit a ridge regression and select the \code{n_features} with
the highest absolute weight.
\item \code{"lasso_path"}: Fit a lasso model and choose the \code{n_features} whose lars
path converge to zero the latest.
}
}

The return value of the returned function will be a \code{tibble} encoding the
explanations in a tidy format. The columns are:
\itemize{
\item \code{case}: The case being explained (the rowname in \code{cases})
\item \code{predict_label}: The label with the highest probability as predicted by \code{model}
\item \code{predict_prob}: The probability of \code{predict_label}
\item \code{label}: The label being explained
\item \code{label_prob}: The probability of \code{label} as predicted by \code{model}
\item \code{feature}: The feature used for the explanation
\item \code{weight}: The weight of the feature in the explanation
\item \code{model_r2}: The quality of the model used for the explanation
\item \code{model_intercept}: The intercept of the model used for the explanation
}
}
\description{
This is the main function of the \code{lime} package. It is a factory function
that returns a new function that can be used to explain the predictions made
by black box models. This is a generic with methods for the different data
types supported by lime.
}
\section{Methods (by class)}{
\itemize{
\item \code{character}: Method for explaining text data

\item \code{data.frame}: Method for explaining tabular data
}}

\examples{
\dontrun{
# Explaining a model based on text data

library(text2vec)
library(lime)
library(xgboost)

data(train_sentences)
data(test_sentences)

get.matrix <- function(text) {
  it <- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}

dtm_train = get.matrix(train_sentences$text)

bst <- xgb.train(list(max_depth = 7, eta = 0.1, objective = "binary:logistic",
                 eval_metric = "error", nthread = 1),
                 xgb.DMatrix(dtm_train, label = train_sentences$class.text == "OWNX"),
                 nrounds = 50)

lime(test_sentences[5, text], bst, get.matrix, n_labels = 1,
     number_features_explain = 2, keep_word_position = FALSE)()
}

# Explaining a model based on tabular data
if (requireNamespace("caret", quietly = TRUE)) {
  library(caret)
  iris_test <- iris[1, 1:4]
  iris_train <- iris[-1, 1:4]
  iris_lab <- iris[[5]][-1]

  # Create Random Forest model on iris data
  model <- train(iris_train, iris_lab, method = 'rf')

  # Create explanation function
  expl <- lime(iris_train, model)
  expl(iris_test, n_labels = 1, n_features = 2)
}
}
